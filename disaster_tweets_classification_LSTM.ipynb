{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11837c2c",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/competitions/nlp-getting-started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415403b6",
   "metadata": {},
   "source": [
    "# Проект: \"предсказание типа комментариев: о реальных катастрофах и нереальных катастрофах.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28048a57",
   "metadata": {},
   "source": [
    "# Импортирование библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "42693212",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T12:04:20.055301Z",
     "start_time": "2023-06-17T12:04:20.037450Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Maxim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e20ebd",
   "metadata": {},
   "source": [
    "# Предварительный анализ и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "40f97632",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:13.600524Z",
     "start_time": "2023-06-17T11:59:13.412764Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train = train.drop(columns = ['keyword', 'location'])\n",
    "test = test.drop(columns = ['keyword', 'location'])\n",
    "\n",
    "df = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "88e6d444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:13.644580Z",
     "start_time": "2023-06-17T11:59:13.603512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  target\n",
       "0   1  Our Deeds are the Reason of this #earthquake M...     1.0\n",
       "1   4             Forest fire near La Ronge Sask. Canada     1.0\n",
       "2   5  All residents asked to 'shelter in place' are ...     1.0\n",
       "3   6  13,000 people receive #wildfires evacuation or...     1.0\n",
       "4   7  Just got sent this photo from Ruby #Alaska as ...     1.0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "465f2677",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:13.659616Z",
     "start_time": "2023-06-17T11:59:13.651301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Форма train: (7613, 3)\n",
      "Форма test: (3263, 2)\n",
      "Форма df=train+test: (10876, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Форма train: {}'.format(train.shape))\n",
    "print('Форма test: {}'.format(test.shape))\n",
    "print('Форма df=train+test: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "45ff4d22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:13.720587Z",
     "start_time": "2023-06-17T11:59:13.662390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      7613 non-null   int64 \n",
      " 1   text    7613 non-null   object\n",
      " 2   target  7613 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 178.6+ KB\n",
      "None\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      3263 non-null   int64 \n",
      " 1   text    3263 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 51.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('{}'.format(train.info()), end = '\\n\\n')\n",
    "print('{}'.format(test.info()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd84cf1",
   "metadata": {},
   "source": [
    "Тестовая выборка меток не имеет, поэтому когда будем анализировать все данные без учета меток будем анализировать df = train + test, когда будет происходить анализ с учетом меток(например, их распределение), то будем использовать только train подвыборку. Пропусков в данных нету."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "772db4c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:13.765459Z",
     "start_time": "2023-06-17T11:59:13.725973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер данных после удаления дубликатов: (10876, 3)\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates()\n",
    "print(f\"Размер данных после удаления дубликатов:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4a6f4987",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:13.788014Z",
     "start_time": "2023-06-17T11:59:13.767946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего твитов в данных: 10876\n",
      "Всего уникальных пользователей в данных: 10876\n"
     ]
    }
   ],
   "source": [
    "print(f'Всего твитов в данных:', df.shape[0])\n",
    "print('Всего уникальных пользователей в данных: {}'.format(df['id'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6e5647f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:13.803361Z",
     "start_time": "2023-06-17T11:59:13.791795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего меток твитов в данных: 2\n",
      "Уникальные метки твитов: [1 0]\n",
      "\n",
      "Количество меток каждого типа твитов:\n",
      "0    4342\n",
      "1    3271\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Всего меток твитов в данных: {}'.format(len(train['target'].unique())))\n",
    "print('Уникальные метки твитов: {}'.format(train['target'].unique()), end = '\\n\\n')\n",
    "print('Количество меток каждого типа твитов:\\n{}'.format(train['target'].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06002785",
   "metadata": {},
   "source": [
    "Количество меток каждого типа твитов резко не отличаются, баланс классов примерно соблюдается."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f97c808",
   "metadata": {},
   "source": [
    "# Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85fcbec",
   "metadata": {},
   "source": [
    "Итак, имеется исходный датафрейм df, в котором признак text соответствует твиту определенного пользователя, то есть это тип данных object, прежде чем приступить к работе с этими твитами, нужно нормализовать текст и каждый твит представить в виде набора нормализованных слов, а также убрать слишком часто встречающиеся слова и слишком редкие слова. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d94ae5a",
   "metadata": {},
   "source": [
    "## Понижение регистра слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2a65a013",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:19.154696Z",
     "start_time": "2023-06-17T11:59:19.123100Z"
    }
   },
   "outputs": [],
   "source": [
    "train['text'] = train['text'].str.lower()\n",
    "test['text'] = test['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af809c04",
   "metadata": {},
   "source": [
    "## Исключение бесполезных слов и других бесполезных конструкций"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504510ef",
   "metadata": {},
   "source": [
    "### Исключение чисел и цифр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "14506dfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:19.255380Z",
     "start_time": "2023-06-17T11:59:19.157713Z"
    }
   },
   "outputs": [],
   "source": [
    "def number_remove(text):\n",
    "    line = re.sub('\\d+', '', text)\n",
    "    return line\n",
    "\n",
    "train['new_text'] = train['text'].apply(lambda x: number_remove(x))\n",
    "test['new_text'] = test['text'].apply(lambda x: number_remove(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecceb1e",
   "metadata": {},
   "source": [
    "### Исключение ссылок на сайты и html ссылок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "747e89a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:19.372312Z",
     "start_time": "2023-06-17T11:59:19.261169Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_remove = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_remove.sub(r'', text)\n",
    "train['new_text'] = train['new_text'].apply(lambda x:remove_urls(x))\n",
    "test['new_text'] = test['new_text'].apply(lambda x:remove_urls(x))\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "train['new_text']=train['new_text'].apply(lambda x:remove_html(x))\n",
    "test['new_text']=test['new_text'].apply(lambda x:remove_html(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389bd61f",
   "metadata": {},
   "source": [
    "### Исключение пунктуационных знаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9e3bc640",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:19.489672Z",
     "start_time": "2023-06-17T11:59:19.374092Z"
    }
   },
   "outputs": [],
   "source": [
    "def punct_remove(text):\n",
    "    punct = re.sub(r\"[^\\w\\s\\d]\",\"\", text)\n",
    "    return punct\n",
    "\n",
    "train['new_text']=train['new_text'].apply(lambda x:punct_remove(x))\n",
    "test['new_text']=test['new_text'].apply(lambda x:punct_remove(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f0e413",
   "metadata": {},
   "source": [
    "### Исключение стоп-слов(слов, которые почти не характеризуют специфику конкретного текста)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1614f180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:19.605122Z",
     "start_time": "2023-06-17T11:59:19.489672Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Maxim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "train['new_text']=train['new_text'].apply(lambda x:remove_stopwords(x))\n",
    "test['new_text']=test['new_text'].apply(lambda x:remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92d318c",
   "metadata": {},
   "source": [
    "### Исключение хэштегов(#) и отсылок(@) в твитах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "29282732",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:19.704272Z",
     "start_time": "2023-06-17T11:59:19.607060Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_hash(x):\n",
    "    text=re.sub(r'#\\w+','',x)\n",
    "    return text\n",
    "train['new_text']=train['new_text'].apply(lambda x:remove_hash(x))\n",
    "test['new_text']=test['new_text'].apply(lambda x:remove_hash(x))\n",
    "\n",
    "def remove_mention(x):\n",
    "    text=re.sub(r'@\\w+','',x)\n",
    "    return text\n",
    "\n",
    "train['new_text']=train['new_text'].apply(lambda x:remove_mention(x))\n",
    "test['new_text']=test['new_text'].apply(lambda x:remove_mention(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c509e7c",
   "metadata": {},
   "source": [
    "### Удаление \"длинных пробелов\"(в случае, если при обработке таковые возникли)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ecf4429d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:19.850193Z",
     "start_time": "2023-06-17T11:59:19.705494Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_space(text):\n",
    "    space_remove = re.sub(r\"\\s+\",\" \", text).strip()\n",
    "    return space_remove\n",
    "\n",
    "train['new_text']=train['new_text'].apply(lambda x:remove_space(x))\n",
    "test['new_text']=test['new_text'].apply(lambda x:remove_space(x))\n",
    "\n",
    "train = train.drop(columns=['text']) \n",
    "test = test.drop(columns=['text']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2576731",
   "metadata": {},
   "source": [
    "## Построение словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1384b0ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:19.865817Z",
     "start_time": "2023-06-17T11:59:19.850193Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_dictionary(texts):\n",
    "    \n",
    "    dictionary = {}\n",
    "    idx = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            if word not in dictionary.keys():\n",
    "                dictionary[word] = idx\n",
    "                idx += 1\n",
    "                \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "970ad506",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:19.887539Z",
     "start_time": "2023-06-17T11:59:19.869603Z"
    }
   },
   "outputs": [],
   "source": [
    "def max_sequence_len(texts):\n",
    "    max_len = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        if len(text.split()) > max_len:\n",
    "            max_len = len(text.split())\n",
    "            \n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "45c92652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:19.970377Z",
     "start_time": "2023-06-17T11:59:19.888784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 17110\n",
      "Максимальное количество слов в предложении: 23\n"
     ]
    }
   ],
   "source": [
    "dictionary = build_dictionary(train['new_text'])\n",
    "max_seq_len = max_sequence_len(train['new_text'])\n",
    "\n",
    "print(f'Размер словаря: {len(dictionary)}')\n",
    "print(f'Максимальное количество слов в предложении: {max_seq_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651806f",
   "metadata": {},
   "source": [
    "# Построение предсказательной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7239693c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:19.987405Z",
     "start_time": "2023-06-17T11:59:19.971889Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a2933434",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:20.037431Z",
     "start_time": "2023-06-17T11:59:20.005903Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer(text, max_seq_len, dictionary):\n",
    "        \n",
    "        sequences = []\n",
    "        for x in text:\n",
    "            sequence = [0] * max_seq_len\n",
    "            for idx, word in enumerate(x.split()):\n",
    "                sequence[idx] = dictionary[word]\n",
    "            sequences.append(sequence)\n",
    "            \n",
    "        return np.array(sequences)\n",
    "    \n",
    "    def __init__(self, text, target, max_seq_len, dictionary):\n",
    "        self.x = self.tokenizer(text, max_seq_len, dictionary)\n",
    "        self.y = target.to_numpy()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "26a217f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:40.787452Z",
     "start_time": "2023-06-17T11:59:40.764622Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train[\"new_text\"], train[\"target\"], test_size=0.3, stratify=train[\"target\"])\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dec7bacb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:42.772419Z",
     "start_time": "2023-06-17T11:59:42.551767Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train = CustomDataset(X_train, y_train, max_seq_len, dictionary)\n",
    "valid = CustomDataset(X_valid, y_valid, max_seq_len, dictionary)\n",
    "test = CustomDataset(X_test, y_test, max_seq_len, dictionary)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=False)\n",
    "valid_loader = DataLoader(valid, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a963d6d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T11:59:45.082509Z",
     "start_time": "2023-06-17T11:59:45.061271Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextClassifier(nn.ModuleList):\n",
    "    \n",
    "    \"\"\"\n",
    "    LSTM Network definition\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, BATCH_SIZE, INPUT_SIZE, HIDDEN_DIM, LSTM_LAYERS, DROPOUT):\n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.hidden_dim = HIDDEN_DIM\n",
    "        self.LSTM_layers = LSTM_LAYERS\n",
    "        self.input_size = INPUT_SIZE\n",
    "\n",
    "        self.d = nn.Dropout(DROPOUT)\n",
    "        \n",
    "        self.out_act = torch.nn.Sigmoid()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=self.input_size, embedding_dim=self.hidden_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.LSTM_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        h = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim)).to(device)\n",
    "        c = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim)).to(device)\n",
    "        \n",
    "        torch.nn.init.xavier_normal_(h)\n",
    "        torch.nn.init.xavier_normal_(c)\n",
    "        \n",
    "        out = self.embedding(x.long())\n",
    "        out, (hidden, cell) = self.lstm(out, (h, c))\n",
    "        out = self.d(out)\n",
    "        out = F.relu(self.fc1(out[:,-1,:]))\n",
    "        out = self.d(out)\n",
    "        out = self.out_act(self.fc2(out))\n",
    "\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a7cfc728",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T12:07:10.573349Z",
     "start_time": "2023-06-17T12:04:25.305182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 : train_loss: 0.669883 train_gini: 0.236306 | valid_loss: 0.680236 valid_gini: 0.123665\n",
      "EPOCH 1 : train_loss: 0.644990 train_gini: 0.399235 | valid_loss: 0.677615 valid_gini: 0.179301\n",
      "EPOCH 2 : train_loss: 0.674712 train_gini: 0.481758 | valid_loss: 0.684514 valid_gini: 0.255199\n",
      "EPOCH 3 : train_loss: 0.422069 train_gini: 0.800992 | valid_loss: 0.604795 valid_gini: 0.508942\n",
      "Early Stopping..\n",
      "CPU times: total: 3min 7s\n",
      "Wall time: 31.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "net = TextClassifier(32, len(dictionary)+1, 128, 4, 0.2)\n",
    "net.to(device)\n",
    "# train_writer.add_graph(net, torch.tensor(X_test), verbose=True)\n",
    "\n",
    "learning_rate = 0.01 # Гиперпараметр коэффициент обучения\n",
    "# train_writer.add_text('LERNING_RATE', str(learning_rate))\n",
    "\n",
    "# Оптимизаторы:\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr = learning_rate, momentum = 0.9, dampening=0, weight_decay=0, nesterov = True)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate, betas=(0.9, 0.999), weight_decay=0, eps=1e-10)\n",
    "# optimizer = torch.optim.AdaGrad(net.parameters(), lr = learning_rate, lr_decay=0, initial_accumulator_value=0, weight_decay=0, eps=1e-06)\n",
    "# optimizer = torch.optim.AdaDelta(net.parameters(), lr = learning_rate, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "# optimizer = torch.optim.RMSprop(net.parameters(), lr = learning_rate, alpha=0.99, weight_decay=0, momentum=0, eps=1e-08, centered=False)\n",
    "# optimizer = torch.optim.Rprop(net.parameters(), lr = learning_rate, etas=(0.5, 1.2), step_sizes=(1e-06, 50))\n",
    "\n",
    "# Уменьшает learning_rate оптимизатора, если изменение средней оценки на валидационных батчах не более 0.0001\n",
    "scheduler = ReduceLROnPlateau(optimizer, min_lr = 0.00001, mode = 'min', factor = 0.1, patience = 4, verbose = True)\n",
    "\n",
    "criterion = nn.BCELoss() # Для задачи бинарной классификации\n",
    "# criterion = nn.CrossEntropyLoss() # Для задачи многоклассовой классификации\n",
    "# criterion = nn.MSELoss() # Для задачи регрессии\n",
    "\n",
    "EPOCHS = 30\n",
    "step = 0\n",
    "min_val_score = 0\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    train_metric_score = 0\n",
    "    net.train()\n",
    "    \n",
    "    for features, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        features = features.type(torch.LongTensor)\n",
    "        label = label.type(torch.FloatTensor)\n",
    "        \n",
    "        output = net(features.to(device))\n",
    "        loss = criterion(output, label.to(device).float())\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        gini = 2*roc_auc_score(label, output.detach().cpu().numpy()) - 1\n",
    "        \n",
    "        # train_writer.add_scalar('CrossEntropyLoss', loss, step)\n",
    "        # train_writer.add_scalar('Gini', gini, step)\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        # if step % 50 == 0:\n",
    "        # print('EPOCH %d STEP %d : train_loss: %f train_gini: %f' % (epoch, step, loss, gini))\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    # train_writer.add_histogram('hidden_layer1', net.linear1.weight.data, step) # Логирование распределения весов первого скрытого слоя\n",
    "    # train_writer.add_histogram('hidden_layer2', net.linear2.weight.data, step) # Логирование распределения весов второго скрытого слоя\n",
    "    # train_writer.add_histogram('hidden_layer3', net.linear3.weight.data, step) # Логирование распределения весов третьего скрытого слоя\n",
    "    # train_writer.add_histogram('hidden_layer4', net.linear4.weight.data, step) # Логирование распределения весов четвертого скрытого слоя\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    train_losses = []\n",
    "    train_scores = []\n",
    "    train_metric_score = 0\n",
    "    with torch.no_grad():\n",
    "        for features, label in train_loader:\n",
    "            output = net(features.to(device).float())\n",
    "            # Calculate error and backpropagate\n",
    "            loss = criterion(output, label.to(device).float()).detach().cpu().numpy()\n",
    "            train_losses.append(loss)\n",
    "            \n",
    "            train_gini = 2*roc_auc_score(label, output.detach().cpu().numpy()) - 1\n",
    "            train_scores.append(train_gini)\n",
    "    \n",
    "    val_losses = []\n",
    "    val_scores = []\n",
    "    val_metric_score = 0\n",
    "    with torch.no_grad():\n",
    "        for features, label in valid_loader:\n",
    "            output = net(features.to(device).float())\n",
    "            # Calculate error and backpropagate\n",
    "            loss = criterion(output, label.to(device).float()).detach().cpu().numpy()\n",
    "            val_losses.append(loss)\n",
    "            \n",
    "            valid_gini = 2*roc_auc_score(label, output.detach().cpu().numpy()) - 1\n",
    "            val_scores.append(valid_gini)\n",
    "            \n",
    "    scheduler.step(np.mean(val_losses))\n",
    "    \n",
    "    if round(np.mean(val_scores), 4) <= min_val_score:\n",
    "        print(\"Early Stopping..\")\n",
    "        break\n",
    "    \n",
    "    if min_val_score < round(np.mean(val_scores), 4):\n",
    "        min_val_score = round(np.mean(val_scores), 4)\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        \n",
    "    if patience > 4:\n",
    "        print(\"Early Stopping..\")\n",
    "        break\n",
    "    \n",
    "    print('EPOCH %d : train_loss: %f train_gini: %f | valid_loss: %f valid_gini: %f' % (epoch, np.mean(train_losses), np.mean(train_scores), np.mean(val_losses), np.mean(val_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cec531ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model, data_loader):\n",
    "    Y_true, Y_preds = [], []\n",
    "    \n",
    "    for X, Y in data_loader:\n",
    "        preds = model(X)\n",
    "        Y_preds.append(preds)\n",
    "        Y_true.append(Y)\n",
    "        \n",
    "    Y_preds, Y_true = torch.cat(Y_preds), torch.cat(Y_true)\n",
    "    \n",
    "    return Y_true.detach().numpy(), Y_preds.detach().numpy()\n",
    "\n",
    "\n",
    "Y_true, Y_preds = get_prediction(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "7cf9883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, precision_score, recall_score, classification_report, roc_auc_score\n",
    "\n",
    "def get_predictions_quality_metrics(Y_true, Y_preds):\n",
    "    Y_preds_binary = [1 if x > 0.5 else 0 for x in Y_preds]\n",
    "    \n",
    "    metrics_binary_tag = ['accuracy_score', 'f1_score', 'log_loss', 'precision_score', 'recall_score']\n",
    "    metrics_binary_func = [accuracy_score, f1_score, log_loss, precision_score, recall_score]\n",
    "    metrics_binary_result = {}\n",
    "    \n",
    "    for i in range(len(metrics_binary_tag)):\n",
    "        metrics_binary_result[metrics_binary_tag[i] + \"{train}\"] = metrics_binary_func[i](Y_preds_binary, Y_true)\n",
    "        print(metrics_binary_tag[i] + \" = \" + f\"{metrics_binary_func[i](Y_preds_binary, Y_true)}\")\n",
    "        \n",
    "    print()\n",
    "    print(classification_report(Y_preds_binary, y_test))\n",
    "    print()\n",
    "    \n",
    "    roc_auc = roc_auc_score(Y_true, Y_preds)\n",
    "    \n",
    "    print(\"roc_auc_score = \" + f\"{roc_auc}\")\n",
    "    print(\"gini_score = \" + f\"{2*roc_auc - 1}\")\n",
    "\n",
    "    return metrics_binary_result, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "13eed2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score = 0.6882661996497373\n",
      "f1_score = 0.6596558317399617\n",
      "log_loss = 11.236025049497115\n",
      "precision_score = 0.7055214723926381\n",
      "recall_score = 0.6193895870736086\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.75      0.71       585\n",
      "           1       0.71      0.62      0.66       557\n",
      "\n",
      "    accuracy                           0.69      1142\n",
      "   macro avg       0.69      0.69      0.69      1142\n",
      "weighted avg       0.69      0.69      0.69      1142\n",
      "\n",
      "\n",
      "roc_auc_score = 0.7646508015545681\n",
      "gini_score = 0.5293016031091362\n"
     ]
    }
   ],
   "source": [
    "metrics_binary_result, roc_auc_test = get_predictions_quality_metrics(Y_true, Y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16080e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b425e6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1008d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
